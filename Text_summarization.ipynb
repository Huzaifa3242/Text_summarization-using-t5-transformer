{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrQ8dVzdEnWIdbqOLcj6OG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Huzaifa3242/Text_summarization-using-t5-transformer/blob/main/Text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZOd9VwhO5KKl"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required libraries\n",
        "# pip install datasets pandas nltk\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Step 2: Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "# Step 3: Load the dataset\n",
        "# Load with specific config (snapshot)\n",
        "dataset = load_dataset(\"permutans/fineweb-bbc-news\", \"CC-MAIN-2020-10\")\n",
        "\n",
        "\n",
        "# Step 4: Convert to DataFrame\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# Step 5: Define preprocessing function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Step 6: Create final DataFrame\n",
        "final_df = pd.DataFrame({\n",
        "    'article': df['text'],\n",
        "    'summary': df['text'].apply(preprocess_text)\n",
        "})\n",
        "\n",
        "# Optional: Save to CSV\n",
        "# final_df.to_csv(\"bbc_news_preprocessed.csv\", index=False)\n",
        "\n",
        "# Preview\n",
        "print(final_df.head())"
      ],
      "metadata": {
        "id": "X5FN5R8S5OHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df=df.iloc[0:15000,:]"
      ],
      "metadata": {
        "id": "DvH80B2-5Q54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=final_df.dropna()\n",
        "df=df.drop_duplicates()\n",
        "df.columns=[\"text\",\"summary\"]"
      ],
      "metadata": {
        "id": "QFuYO4en5Q1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text\"]=df[\"text\"].str.encode('ascii','ignore').str.decode('ascii')\n",
        "df[\"summary\"]=df[\"summary\"].str.encode('ascii','ignore').str.decode('ascii')"
      ],
      "metadata": {
        "id": "vZb_ZalM6kjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers"
      ],
      "metadata": {
        "id": "9EmEwSki5QyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet pytorch-lightning"
      ],
      "metadata": {
        "id": "WfThd66i5Qug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "sO8LEt3Z5Qre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast as T5Tokenizer\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "4qGz6X1H5Qom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df,test_df=train_test_split(df,test_size=0.2)"
      ],
      "metadata": {
        "id": "6LrmIUfc5Qel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummaryDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        tokenizer: T5Tokenizer,\n",
        "        text_max_token_len: int = 512,\n",
        "        summary_max_token_len: int = 128,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.text_max_token_len = text_max_token_len\n",
        "        self.summary_max_token_len = summary_max_token_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index:int):\n",
        "        data_row=self.data.iloc[index]\n",
        "        text=data_row[\"text\"]\n",
        "\n",
        "        text_encodings= self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.text_max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        summary_encodings= self.tokenizer(\n",
        "            data_row[\"summary\"],\n",
        "            max_length=self.summary_max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels=summary_encodings[\"input_ids\"]\n",
        "        # Change: Use tokenizer.pad_token_id instead of -100\n",
        "        labels[labels==0]=self.tokenizer.pad_token_id\n",
        "\n",
        "        return dict(\n",
        "            text=text,\n",
        "            summary=data_row[\"summary\"],\n",
        "            text_input_ids=text_encodings[\"input_ids\"].flatten(),\n",
        "            text_attention_mask=text_encodings[\"attention_mask\"].flatten(),\n",
        "            labels=labels.flatten(),\n",
        "            labels_attention_mask=summary_encodings[\"attention_mask\"].flatten()\n",
        "        )"
      ],
      "metadata": {
        "id": "poXXScnk5QSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummaryDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_df: pd.DataFrame,\n",
        "        test_df: pd.DataFrame,\n",
        "        tokenizer: T5Tokenizer,\n",
        "        batch_size: int = 8,\n",
        "        text_max_token_len: int = 512,\n",
        "        summary_max_token_len: int = 128,\n",
        "    ):\n",
        "      # Fix: Provide the required arguments to super().__init__()\n",
        "      super(SummaryDataModule, self).__init__()\n",
        "      self.train_df=train_df\n",
        "      self.test_df=test_df\n",
        "      self.batch_size=batch_size\n",
        "      self.tokenizer=tokenizer\n",
        "      self.text_max_token_len=text_max_token_len\n",
        "      self.summary_max_token_len=summary_max_token_len\n",
        "\n",
        "\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset=SummaryDataset(\n",
        "            self.train_df,\n",
        "            self.tokenizer,\n",
        "            self.text_max_token_len,\n",
        "            self.summary_max_token_len\n",
        "        )\n",
        "\n",
        "        self.test_dataset=SummaryDataset(\n",
        "            self.test_df,\n",
        "            self.tokenizer,\n",
        "            self.text_max_token_len,\n",
        "            self.summary_max_token_len\n",
        "        )\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False, # Fix: shuffle should be False for test_dataloader\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False, # Fix: shuffle should be False for val_dataloader\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )"
      ],
      "metadata": {
        "id": "DBiuUXE35zGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=10\n",
        "BATCH_SIZE=16\n",
        "data_module=SummaryDataModule(train_df,test_df,tokenizer)"
      ],
      "metadata": {
        "id": "rs2xEQRj5zBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "\n",
        "class SummaryModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n",
        "        output = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            decoder_attention_mask=decoder_attention_mask\n",
        "        )\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"text_input_ids\"]\n",
        "        attention_mask = batch[\"text_attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "        loss, outputs = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_attention_mask=labels_attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"text_input_ids\"]\n",
        "        attention_mask = batch[\"text_attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "        loss, outputs = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_attention_mask=labels_attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"text_input_ids\"]\n",
        "        attention_mask = batch[\"text_attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "        loss, outputs = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_attention_mask=labels_attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        from torch.optim import AdamW\n",
        "        return AdamW(self.parameters(), lr=1e-4,weight_decay=0.01)"
      ],
      "metadata": {
        "id": "OnLEyn8o5y_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=SummaryModel()"
      ],
      "metadata": {
        "id": "2qXofHfA5y9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./lightening_logs"
      ],
      "metadata": {
        "id": "5fXPHCnk5y6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "# ModelCheckpoint: saves best model based on val_loss\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"checkpoints\",\n",
        "    filename=\"best-checkpoint\",\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=2,\n",
        "    verbose=True,\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "\n",
        "# TensorBoard Logger (for visual logging)\n",
        "tensorboard_logger = TensorBoardLogger(\"lightening_logs\", name=\"bbc-news-summary\")\n",
        "\n",
        "# CSV Logger (for table-style metrics logging)\n",
        "csv_logger = CSVLogger(\"logs\", name=\"bbc-news-summary\")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    devices=1,\n",
        "    max_epochs=EPOCHS,\n",
        "    logger=[tensorboard_logger, csv_logger],\n",
        "    callbacks=[checkpoint_callback, early_stop_callback],\n",
        "    enable_model_summary=True,\n",
        "    detect_anomaly=True,\n",
        "    enable_progress_bar=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "N77d_d1k5y3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model,data_module)"
      ],
      "metadata": {
        "id": "Tfw5F2BW5_Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model=SummaryModel.load_from_checkpoint(\n",
        "    trainer.checkpoint_callback.best_model_path\n",
        ")\n",
        "trained_model.freeze()"
      ],
      "metadata": {
        "id": "8CiwXHai5-_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import length_hint\n",
        "def summarize_text(text):\n",
        "  text_encodings=tokenizer(\n",
        "      text,\n",
        "      max_length=512,\n",
        "      padding=\"max_length\",\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "  # Move tensors to the same device as the model\n",
        "  text_encodings = {k: v.to(trained_model.device) for k, v in text_encodings.items()}\n",
        "\n",
        "\n",
        "  generate_ids=trained_model.model.generate(\n",
        "      input_ids=text_encodings[\"input_ids\"],\n",
        "      attention_mask=text_encodings[\"attention_mask\"],\n",
        "      max_length=200, # Fix: Corrected the typo from 'max_lenth' to 'max_length'\n",
        "      num_beams=2,\n",
        "      repetition_penalty=2.5,\n",
        "      length_penalty=1.0,\n",
        "      early_stopping=True\n",
        "  )\n",
        "\n",
        "  preds=[\n",
        "      tokenizer.decode(gen_id,skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "      for gen_id in generate_ids\n",
        "  ]\n",
        "  return \"\".join(preds)"
      ],
      "metadata": {
        "id": "yGKlsJjb5-8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "bN4roWnA5-4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "preds = []\n",
        "refs = []\n",
        "\n",
        "# Loop through the test DataFrame\n",
        "for _, row in test_df.iterrows():\n",
        "    text = row[\"text\"]\n",
        "    reference_summary = row[\"summary\"]  # Make sure this column exists!\n",
        "\n",
        "    # Generate prediction using your custom function\n",
        "    generated_summary = summarize_text(text)\n",
        "\n",
        "    preds.append(generated_summary)\n",
        "    refs.append(reference_summary)\n",
        "\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# Compute the results\n",
        "results = rouge.compute(predictions=preds, references=refs)\n",
        "\n",
        "# Print scores\n",
        "for key in results:\n",
        "    print(f\"{key}: {results[key]}\")"
      ],
      "metadata": {
        "id": "Hiv_j5qI6fAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yK9cW-id6e9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}